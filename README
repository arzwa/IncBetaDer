
---

First, to give some context: my use case is equivalent to the following contrived example, in which I would like to define a probabilistic program that uses a discretized Beta mixture model.

```julia
@model themodel(X, K) = begin
    a ~ Exponential()
    b ~ Exponential()
    p = discretize(Beta(a, b), K)
    X ~ MixtureModel([Bernouilli(p[i]) for i=1:K])
end

function discretize(d, K)
    qstart = 1.0/2K
    qend = 1. - 1.0/2K
    xs = quantile.(d, qstart:(1/K):qend)
    xs *= mean(d)*K/sum(xs)  # rescale by factor mean(d)/mean(xs)
end
```

Now, this does not work, because currently the quantile function for the Beta distribution (which is the inverse of the incomplete Beta function, or `beta_inc_inv` in `SpecialFunctions.jl`) does not work with AD. (This brings me to a first question, why do many methods in `SpecialFunctions` only take `Float64` type args?)

Now first I experimented (lazily) with adapting the arg types for the relevant `SpecialFunctions` methods and using AD to get my gradient of interest, but this gets too slow. 

Then I've implemented an algorithm for the gradient of the CDF of the beta distribution (i.e. `beta_inc`) with respect to the `a` and `b` parameters (which I found [here](https://www.jstatsoft.org/article/view/v003i01)). This gives me the gradient of `beta_inc(a, b, x)` w.r.t. `a` and `b` fast.  I would expect that this would enable faster computations of the `beta_inc_inv` gradient (which mainly calls `beta_inc` in a numerical inversion routine if I understand correctly). 

However, now I'm not sure how to proceed (I am no analysis guru, and all the stuff in `JuliaDiff` is a bit daunting). It seems to me I should implement some diffrule for `beta_inc`? Say my function is `beta_inc_grad(a,b,x)` and returns a tuple with `beta_inc(a,b,x)` and its partial derivatives w.r.t. `a` and `b`, is 

```julia
@define_diffrule SpecialFunctions.beta_inc(a, b, x) =
    :( beta_inc_grad($a, $b, $x)[2] ), :( beta_inc_grad($a, $b, $x)[3] ), :NaN
```

what I should implement to make this work with AD? (Also, would this call`beta_inc_grad` twice?) 

---

I got some time to try and look further into this. ChainRules(Core) seems to be the package relevant to defining new AD primitives. I tried to follow the [ChainRules docs](https://www.juliadiff.org/ChainRulesCore.jl/dev/index.html) to define a 'forward rule' for `beta_inc`, based on my understanding I thought it should look like this

```julia
function frule((_, Δp, Δq, Δx), ::typeof(SpecialFunctions.beta_inc), p, q, x)
    z, ∂p, ∂q, ∂x = beta_inc_grad(p, q, x)
    Ω = (z, 1. - z)  # beta_inc returns a tuple (z, 1. z) (not sure why)
    ∂Ω1 = (∂p, ∂q, ∂x)
    ∂Ω2 = (-∂p, -∂q, -∂x)
    ∂Ω = Composite{typeof(Ω)}(∂Ω1, ∂Ω2)
    return (Ω, ∂Ω)
end
```

Although I'm unsure about the wrapping of the tuples in `∂Ω`. This seems to work (perhaps it needs some further checks on numerical accuracies, but that's not the issue ATM)

```julia
p, q, x = 0.2, 0.3, 0.1
primal = beta_inc(p, q, x)
g1 = grad(central_fdm(5,1), y->beta_inc(y...)[1], [p, q, x])[1]
g2 = grad(central_fdm(5,1), y->beta_inc(y...)[2], [p, q, x])[1]
Ω, ∂Ω = frule((Zero(), 1., 1., 1.), beta_inc, p, q, x)
@info "results" primal Ω g1 g2 ∂Ω[1] ∂Ω[2]
```

```
┌ Info: results
│   primal = (0.41213400436324327, 0.5878659956367567)
│   Ω = (0.41213400436324304, 0.5878659956367569)
│   g1 =
│    3-element Array{Float64,1}:
│     -1.618217849311664
│      0.6270147933301474
│      0.876624590752917
│   g2 =
│    3-element Array{Float64,1}:
│      1.6182178493113217
│     -0.6270147933302119
│     -0.8766245907528432
│   ∂Ω[1] = (-1.6182190459639405, 0.6270147933298197, 0.8766245907535154)
└   ∂Ω[2] = (1.6182190459639405, -0.6270147933298197, -0.8766245907535154)
```

But now I'm still unclear how I can use this rule with ForwardDiff? Would someone from the JuliaDiff community (@oxinabox?) mind giving me some further directions?

---
